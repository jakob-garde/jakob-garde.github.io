<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-02-02T09:32:31+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Projects</title><subtitle>Systems development</subtitle><entry><title type="html">Good Code Evolves from Clunky Code</title><link href="http://localhost:4000/2026/01/26/evolves.html" rel="alternate" type="text/html" title="Good Code Evolves from Clunky Code" /><published>2026-01-26T00:00:00+01:00</published><updated>2026-01-26T00:00:00+01:00</updated><id>http://localhost:4000/2026/01/26/evolves</id><content type="html" xml:base="http://localhost:4000/2026/01/26/evolves.html"><![CDATA[<p>Ever thought to yourself; <em>Hold on, this code is objectively bad.</em> But you still don’t feel bad about it? Then you might be on the right track to writing better code!</p>

<h2 id="programming-is-a-lot-like-writing">Programming is a lot like writing</h2>

<p>There’s the well-known trick in writing of <em>don’t edit, but edit later</em>. The idea is to get a draft on the page that can be improved into the final document.</p>

<p>In programming we have similar processes, for example code compression or <a href="https://caseymuratori.com/blog_0015">semantic compression</a>. Others like to talk about refactoring, or say <em>iterate and integrate</em>. In Denmark, a computer science meme demands that code must be written three times to be good:</p>

<p>When learning new things, experimenting with techniques and searching in broad strokes, we just don’t know how best to write our program.</p>

<p>Once we have written that first version, we see major vectors of improvement. This calls for a re-write which results in a much better version-2, but having written the program twice, we now finally see the perfect third way, a flexible and simple path that solves everything elegantly. This calls for yet another re-write. Only when that is complete, will the program be considered as good as it can get (…)</p>

<p>Product owners tend to cringe when programmers talk about a re-write, and programmers tend to cringe when product owners request yet another un-planned feature update to legacy pasta. Of course, nobody wants un-maintainable spaghetti code.</p>

<p>Here is a possible path to badly structured code: The project was going great, we moved fast and iterated on an early version of code that the programmer hadn’t written multiple times before. This lead to bad, early decisions getting locked-in for life.</p>

<p>Therefore we should consider front-loading some rewrites and refactoring work, long before it becomes absolutely necessary - or impossible.</p>

<h2 id="procedural-map-generator">Procedural map generator</h2>

<p>We are not getting into how it works. Instead, we are going to look at the process that brought the code from ugly to much better.</p>

<p>Recently I have been dabbling in game development using the graphics librari <a href="https://www.raylib.com/cheatsheet/cheatsheet.html">raylib</a>. It is not a full game engine so I get to learn how to write all juicy game logics myself. Procedural generation is very intesting and accessible, so I wrote a random map generator using wavefunction collapse.</p>

<p>Briefly, tiles are in a “superposition” of all outcomes. With each iteration, possible outcomes are reduced as neighbouring elements “collapse” into specific states and the map is complete. Eventually, every tile is resolved into one specific type.</p>

<p>Here’s a demo: <a href="https://jakobgarde.itch.io/endless-forest-map-procgen-demo">wfc procgen</a></p>

<p><img src="/assets/procgen_finished.png" alt="wfc procgen" /></p>

<h2 id="five-stages-of-code">Five stages of code</h2>

<p>The implementation is only a few hundred lines, but it went through at least five stages of development over a few days:</p>

<ul>
  <li>First working implementation</li>
  <li>Total re-write</li>
  <li>Refinement, abstraction</li>
  <li>Extension with new features</li>
  <li>Integration into the application environment</li>
</ul>

<p>The end goal is a unit or mini-library that depends only on base layers. It should be easy for the app as a whole to integrate, and allow for straight-forward testing and porting to other projects.</p>

<p>The initial stage of getting the code working is crucial. That stage is called a “working implementation”, and it positions us very well for refining it into better code. By that mean code that is simpler, smaller, more readable and easy to test and debug.</p>

<p>There is no way to “write code faster, better, the first time” without first writing that embarassingly ugly first working implementation.</p>

<p>Consider Gall’s Law: “a working complex system is invariably found to have evolved from a simple system that worked”. 1970’s systems theory, a favourite decade of mine.</p>

<p>We could ship this first version if we didn’t need more features on it - but as noted above, this is where spaghetti code comes from. Can we live with that? Maybe. Probably not.</p>

<h3 id="stage-1-the-first-working-version">Stage 1: <em>The first, working version</em></h3>

<p>We write the thing in the most direct or immediate way that comes to mind. The goal is to bring the code into a working state without using abstractions beyond the bare minimum. We shouldn’t try and think ahead at this point, just get it working.</p>

<p>Some basic test code is needed so that we can see what happens. In my case, I had a random map on the left, and the generated map on the right. I would step through iterations of the algorithm by pressing SPACE, and debug info was printed directly into the map on the screen as needed. This test method was carried over into consecutive stages.</p>

<p>Code: <a href="https://github.com/jakob-garde/endless/blob/main/article/wfc_stage_1.h">wfc_stage_1.h</a></p>

<pre>
/*
First implementation:
- It works: A great starting point!
- The alg creates a tile map where flowers and rock tiles can not be adjacent
- Explicit, clunky implementation that is specific and verbose
- Tile superposition described as a clunky list of options
- About ~300 lines

Problems:
- Difficult to extend, multiple code edits requred
- Not usable, act as a learning step
- Out-commented code here and there
*/
</pre>

<p><img src="/assets/procgen_debug.png" alt="wfc procgen" /></p>

<h3 id="stage-2-total-rewrite-from-scratch">Stage 2: <em>Total rewrite from scratch</em></h3>

<p>With the experience gainned from writing the initial version, we can now do it all again. Instead of modifying and debugging, we just start over. This allows the design the data and code structures to alleviate some of the pitfalls we encountered writing the first implementation.</p>

<p>The second stage should give the same output as the first.</p>

<p>Code: <a href="https://github.com/jakob-garde/endless/blob/main/article/wfc_stage_2.h">wfc_stage_2.h</a></p>

<pre>
/*
Total re-implementation:
- Much clearer how the algorithm works
- Reduction matrix to describe what tiles can not be next to each-other
- Tile superposition described as an array of bools
- Grid initialization using one function; written to passed memory arena
- About ~200 lines

Problems:
- Clunky way to iterate the grid when "reducing" neighouring tile options
- Clunky way to get neighbouring tiles
- No recursive reduction will cause a bug in the future
- Reduction matrix isn't abstracted correctly*/
</pre>

<h3 id="stage-3-refactor-and-isolate-complexity-hotspots">Stage 3: <em>Refactor and isolate complexity hotspots</em></h3>

<p>We should improve the second implementation version by means of refactoring and code compression.</p>

<p>At this stage the general code structure was ok, the clunkiness now being localized in implementations rather than baked into the superstructure. I prepared for the next stage by experimenting with the way that option exclusion was encoded (a complexity hotspot). Experiments also revealed the need for recursive update of auto-collapsed tiles.</p>

<p>Code: <a href="https://github.com/jakob-garde/endless/blob/main/article/wfc_stage_3.h">wfc_stage_3.h</a></p>

<pre>
/*
Refactoring, finding serious bugs and refining:
- Adjacency matrix replaces "reducion" matrix
- Adjacency matrix has Directional adjacency rules (although they have no effect at this stage)
- Cleaner way to iterate tile neighbours: Using "kernel" arrays
- Recursive collapse of tiles that reach entropy == 1 (one option) to update their neighbours
- Uses much better variable names
- Clearly separated lib, user and debug-draw sections
- About ~220 lines from this point on

Problems: 
- Has the final serious bug: We collapse high-entropy tiles (incorrect) instead of low-entropy tiles
*/
</pre>

<h3 id="stage-4-extend-with-new-features">Stage 4: <em>Extend with new features</em></h3>

<p>We now have a solid foundation to build out the necessary complexity for implementing the features we actually want.</p>

<p>Up until this stage, I was only working with three map tile types: Grass, flowers and rocks. Flowers and rocks could not be next to eachother, Now it was time to add five new til types: The forest tiles are: <em>up, down, left, right</em> and <em>inner</em>.</p>

<p>Adding the adjacency rules for these extra tile types revealed a serious bug: When it selected a random tile for collapse, it chose those which had the most options available. The contracty is intuitively correct, it should collapse the tiles with the least options / most constraints first. The generator worked almost perfectly after that change.</p>

<p>Finally the map with meadows and forest patches was generated correctly, I was happy with the features and looked to wrap it up.</p>

<p>Code: <a href="https://github.com/jakob-garde/endless/blob/main/article/wfc_stage_4.h">wfc_stage_4.h</a></p>

<pre>
/*
Directional adjacency rules applied to procgen a map with connected forest tiles
- Directional adjacency rules active and working
- Activate and implement five forest tile types
- Select tiles to collapse by lowest-entropy rather than highest
- About 220 lines for the separated "lib" code section
- Correctly selects tiles to collapse using min-entropy
- Everything seems to work perfectly

Problems:
- Not fit for integration into an app - global variables, type names are too generic, no explicit draw function
- Adjacency rules are concise, but a little clunky to define (dontfix)
- Rare placement bug
*/
</pre>

<h3 id="stage-5-integration-into-the-code-base">Stage 5: <em>Integration into the code base</em></h3>

<p>It is now time to think of the broader picture. How specific or general is this code? Are we using general names that might be misunderstood in the future?</p>

<p>For example, “Grid” should be named “WFCGrid” because it doesn’t have any broader application. Also got rid of global variables that were convenient during implementation, but would certainly get in the way down the line. Global variables, badly implemented function and short-hand variable names: None of it really matters until we integrate / ship.</p>

<p>Code: <a href="https://github.com/jakob-garde/endless/blob/main/article/wfc_stage_5.h">wfc_stage_5.h</a></p>

<pre>
/*
Integration:
- Rename types to be more specific
- Do not depend on global variables
- One function to run the algorithm (lib section)
- Function to apply draw-able frame object to tiles, a post-run step (draw section)
- Local user section is instead a few debug functions for potential future use
- User code is now in the app, we are calling it from the outside 
- Implementation of lib code is still about ~220 lines
- Draw function was copy-pasted and adapted in user code 

Problems: Can be changed later; and we are moving ahead at this point
- Wanted refactor: Tile type is not great for use in the app
- Wanted refactor: More convenient way of defining adjacency rules
- Wanted extension: A way to stitch separately generated regions created by the algorithm
- Wanted bugfix: Rare inconsistent placements can occur (research)
*/
</pre>]]></content><author><name></name></author><summary type="html"><![CDATA[Ever thought to yourself; Hold on, this code is objectively bad. But you still don’t feel bad about it? Then you might be on the right track to writing better code!]]></summary></entry><entry><title type="html">Building on Legacy</title><link href="http://localhost:4000/2025/12/07/mctrace.html" rel="alternate" type="text/html" title="Building on Legacy" /><published>2025-12-07T00:00:00+01:00</published><updated>2025-12-07T00:00:00+01:00</updated><id>http://localhost:4000/2025/12/07/mctrace</id><content type="html" xml:base="http://localhost:4000/2025/12/07/mctrace.html"><![CDATA[<h1 id="mctrace">mctrace</h1>

<p><em>A few years ago I worked on the open-source software package McStas at DTU Physics (2015-2020). For the official project, see mccode.org. Recently I have been re-visiting the software package as a fun hobby and learning experience. The following is a reflection of my own personal opinions, and nothing more.</em></p>

<p>Legacy software sometimes gets burried in layers of newer software.</p>

<p>That’s certainly a good thing, because changing what’s underneath can be risky. In the case of physics simulation code, obscure and disastrous 
bugs could be introduced. We’d rather build new features.</p>

<h2 id="what-if-re-visiting-old-code-is-exactly-what-unlocks-new-possibilities">What if re-visiting old code is exactly what unlocks new possibilities?</h2>

<p>This project is a vertical slice. It is not an attempt to re-implement
existing functionality or achieve feature-parity as a baseline. Rather, it’s an attempt to show what is possible.</p>

<p>It combines the functionality of three or four existing tools into one, and adds a number of new features and many more possibilities. It loads and runs much faster and distribution is simple.</p>

<p>Above all, it shows how to build completely new functionality, including features that would be very inconvenient to get in other ways.</p>

<p>The mctrace binary compiles from source and takes up a few MB for the executable, which is distributed along with a thin platform abstraction layer (glew/glfw). Due to the necessity of compiling on Windows with gcc, they couldn’t be statically linked, but are included in the zip.</p>

<ul>
  <li>Source: <a href="https://github.com/jakob-garde/mctrace">mctrace</a></li>
  <li>Win-x64 binary: <a href="https://github.com/jakob-garde/mctrace/raw/refs/heads/main/release/x64-win/mctrace-0.1.0.zip">mctrace-win</a></li>
</ul>

<p>Essentially this is a port of the neutron scattering instrument called PSI_DMC and its fifteen components. Porting more to the mctrace format is not an issue, since the generic pipeline is there. As mentioned above however, this is a tech demo / vertical slice, not a complete port, so I focused on this simple and illustrative instrument for the project.</p>

<p>Code generation using my mcparse generator:</p>

<ul>
  <li>Source: <a href="https://github.com/jakob-garde/mcparse">mcparse</a></li>
</ul>

<p>The essential port takes place under the hood, but the most visible and attention-demanding is the visualization, and here’s some fun examples:</p>

<h3 id="example-mouse-hover-and-selection-boxes">Example: Mouse-hover and selection boxes</h3>

<p>When moving the mouse across the screen above a component, a box will appear around it, and its name is displayed in a tool-tip. By left-clicking, the component will become selected. This causes an info box to appear listing its name, type and parameter values.</p>

<p>When the Monitors tab is selected, a 1D or 2D plot is also displayed, showing current trace data. (NOTE: To see the plots, start the particle trace in the Simulation tab.)</p>

<p><img src="/assets/10_mon_1D.png" alt="Sample monitor" style="" /></p>

<h3 id="example-coloured-rays-by-lambda">Example: Coloured rays by Lambda</h3>

<p>As a fun visualization effect, I coloured the neutron trajectories by wavelength, which gives a nice effect when looking at the monochromator. (The monochromator is akin to a “prism” for visible light.)</p>

<p>You may notice that while rays of many colours hit the monochromator, only certain colours escape towards the sample component.</p>

<p><img src="/assets/04_mono.png" alt="Monochromator-autumn" style="" />
<img src="/assets/04b_monochromator_jet.png" alt="Monochromator-jet" style="" /></p>

<h3 id="example-visualize-particles-that-reach-a-component">Example: Visualize particles that reach a component</h3>

<p>If the trace button has been clicked in the Simulation tab, neutron trajectories will be shown in the trace tab. The shown “rays”/trajectories are a random selection of the particles that reach the selected component.</p>

<p>This lets us investigate sub-sets of the traced rays by clicking on various components. For example, try clicking the detector (the large banana-shaped component at the end of the instrument) to see a fan of rays escaping the sample.</p>

<p><img src="/assets/05_sample_ray.png" alt="Sample beam" style="" />
<img src="/assets/06_overview_detector.png" alt="Scattered" style="" /></p>

<h1 id="list-of-features">List of Features</h1>

<p>Below is a tentative list of the notable features implemented in this project. It is separated into three sections, showcasing the simulation core, visualization and code structure.</p>

<h2 id="simulation-engine">Simulation engine</h2>

<p>The legacy mccode simulation engine infrastructure has been re-implemented and modernized.</p>

<p>1) Components (tested on 15 ported components)</p>

<ul>
  <li>generated .h files, one for each .comp file, wrapping create/init/trace/save/finally functions</li>
  <li>component base class (/header struct) used by the app layer for all component access</li>
  <li>generic parameter info available from the base component struct</li>
  <li>types: ported 15 components to C++</li>
  <li>type-agnostic wrapper for the above calls in a comps_meta.h</li>
  <li>transforms translation from 4x4 to legacy sim-compatible 3x3 + position</li>
</ul>

<p>2) Instruments (tested for the PSI_DMC configurarion)</p>

<ul>
  <li>one generated instument .h file for each .instr file with create/init/configure/finally functions</li>
  <li>generated specific instrument struct</li>
  <li>instrument wrapper struct / base class</li>
  <li>generated parameter hooks (name, value ptr)</li>
  <li>instrument initialization and configuration function with:
    <ul>
      <li>component creation and init</li>
      <li>component transforms given instr params, comp params and AT/ROT with relative/absolute</li>
    </ul>
  </li>
</ul>

<p>3) Simulation</p>

<ul>
  <li>particle propagation loop (runs in a worker thread)</li>
  <li>particle propagation pause/reset using the current instrument init</li>
  <li>simulation libraries re-packaged into simcore.h/simlib.h</li>
  <li>component display hooks: A callback fired by all components during DISPLAY</li>
  <li>component plot/monitor data hooks: A callback fired during SAVE</li>
  <li>particle trace “events” hooks: A callback for state/scatter/absorb events</li>
</ul>

<h2 id="visualization-engine">Visualization Engine</h2>

<p>The instrument is viewed as component “display” wireframes in 3D combined with info-boxes.</p>

<p>1) Wireframe display</p>

<ul>
  <li>component wireframes</li>
  <li>full instrument view of all components hovering above a scale grid</li>
  <li>component mouse-selection and next/prev component buttons</li>
  <li>3D camera pan/zoom using the mouse</li>
  <li>dbl-click and enter-press to focus the camera view on the selected component</li>
  <li>mouse hover tooltip showing component name</li>
  <li>component selection infobox with name/type, absolute at/rot and current parameter values</li>
  <li>separate view modes for monitors vs. physical components</li>
  <li>nagivate the UI using left/right/tab/space key strokes</li>
</ul>

<p>2) Monitor plot-data</p>

<ul>
  <li>monitor 2D data written to texture arrays using color maps</li>
  <li>1D and 2D monitor plot with component name and parameter values when selected</li>
  <li>1D and 2D overview plots with component name</li>
  <li>component monitor data reset</li>
</ul>

<p>3) Particle trajectories</p>

<ul>
  <li>particle propagation with pause/reset runs in worker thread</li>
  <li>particle trace recording (state/scatter/absorb events)</li>
  <li>particle trajectories bundled by component-reached (non-biased)</li>
  <li>view particle trajectories by selecting a component</li>
</ul>

<h2 id="structural">Structural</h2>

<p>The app compiles in less than a second with no intermediary build steps, and
it is easy to debug both the legacy component- and engine code, as well as the application layers.</p>

<p>1) Code structure</p>

<ul>
  <li>minimal code-generated sections</li>
  <li>separate legacy, application and visualization layers</li>
  <li>eliminated most legacy globals, hereby enabling consecutive runs of arbitrary simulation configurations</li>
  <li>core simulation runtime libraries extracted into separate units that are included by default</li>
</ul>

<p>2) Memory management</p>

<ul>
  <li>instrument configuration state is layed-out sequentially in a localized “arena” containers</li>
  <li>containers also manage the lifetime of a trace-simulation</li>
  <li>separate instrument configs are managed individually with individual init/finally life cycles</li>
  <li>note that mcstas components often call malloc() during initialization and free() during finalize (unchanged)</li>
</ul>

<p>3) Extensibility</p>

<ul>
  <li>straight-forward to replace/tweak the particle propagation algorithm</li>
  <li>improved optimization potential due to propagation code being accessible for development</li>
  <li>bulk-propagation rather than single-propagation of particles seems an obvious next step</li>
  <li>arbitrary, dynamic particle caching/logging</li>
  <li>dynamic instrument configurations independent of the DSL</li>
  <li>include arbitrary C++ code</li>
</ul>

<p>4) Known Bugs</p>

<ul>
  <li>Monitor_nD component does not capture data,  substituted with Cyl_monitor</li>
  <li>win touchpad scrolling is wonky, mouse wheel works better</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[mctrace]]></summary></entry><entry><title type="html">Manually parsing the McStas DSL</title><link href="http://localhost:4000/2025/10/13/parser.html" rel="alternate" type="text/html" title="Manually parsing the McStas DSL" /><published>2025-10-13T00:00:00+02:00</published><updated>2025-10-13T00:00:00+02:00</updated><id>http://localhost:4000/2025/10/13/parser</id><content type="html" xml:base="http://localhost:4000/2025/10/13/parser.html"><![CDATA[<h1 id="text-parsing-is-fun">Text parsing is fun</h1>

<p><em>For years I worked on the open-source software package McStas as a core developer at DTU Physics (2015-2020), which is why I know so much about the McStas core software. Currently re-visiting the project on a completely volountary and independent basis. All statements below are thus reflections of my personal findings and opinions, and nothing more.</em></p>

<p>Recently while researching and learning more C, I happened upon custom parsing and thought it was pretty cool. Turns out that people have been parsing many things manually since way back when, and done it very well.</p>

<p>Parsing sometimes gets a bad rep afterall, and seems like a thing that “should just work”, something we should have tools for.</p>

<p>Plenty og tools were indeed written during the Bell Labs era and given names like Yacc or (later on) BISON. These were ancestors of the infamous GNU, one of the most hoofed software animal coming out of the 1980’s.</p>

<p>Yet at closer look, parsing is not just about reading JSON files or writing custom compilers (a common practice in those days). It is the first component of one of the most powerful programming paradigms: META programming.</p>

<h1 id="what-is-meta-programming">What is META programming?</h1>

<p>Meta programming means writing a program that writes another program. This is admittedly a two-step process: First you write and execute a program that outputs the source code for another program, which is compiled and executed separately, and often automatically.</p>

<p>With meta programming we may configure certain things abstractly, let the meta process take its course, and reap the automated benefits. This can be much better than writing everything out in tedious detail, in situations where there could be a lot of repetition, or when multiple lines of code is needed to do simple things.</p>

<p>If this sounds a bit complicated, well, it is: Meta programs can become incomprehensible to anyone but their authors. Some resemble convoluted math proofs that skip way too many steps, but I digress.</p>

<p>Despite the potential for obscurity, meta programming comes with enormous power, and allows us to do many things that are quite clever and time-saving.</p>

<p>Were I to explain the importance of meta programming to a non-programmer, I’d point to the widespread usage of scripting languages like Python, JavaScript, Ruby, and Perl. All very popular - and, amongst other things, have the “reflection” programming-language feature which is notably lacking in C.</p>

<p>Therefore, C programmers who wanted to do meta programming had to manually parse configuration files and generate their own code. Sure it took a little longer than writing a Python script, but it was a straight-forward method that worked just fine.</p>

<h1 id="are-we-still-using-that">Are we still using that?</h1>

<p>Also McStas, a physics simulation package originally dating from 1997, was indeed written by people wielding the power of META. As a physics simulation, performance requirements dictated that it was written in C. Thus the meta program had to output C code.</p>

<p>Why not write the meta program in C, too? This was a dependable and portable, and from a certain perspective, a pretty great choice. However first, we should spend a little more with those software animals mentioned above, specifically BISON and FLEX.</p>

<p>The BISON/FLEX combo is known as a parser generator. It is not just a parser, not just a code generator, but both, and makes meta programming in C easier. A tool that reads in configuration files and outputs C source-code for a program that parses text according to specifications in those configuration files.</p>

<h1 id="overly-contrived-example">Overly contrived example</h1>

<p>In the BISON/FLEX system you write rules that dictate how the meta program / an input text file should be structured. You specify “grammar rules” for the allowed words, sentences and sequences of the DSL.</p>

<p>For example, if we wanted to parse emails, we could say that it should contain the following sections:</p>

<pre>
"email: greeting body farewell"
</pre>

<p>Breaking it down further, the individual elements should be defined as well:</p>

<pre>
"greeting: formality name"
"body: TEXT"
"farewell: formality name"
</pre>

<p>And even further:</p>

<pre>
"formality: 'Hello' or 'Dear'"
"body: TEXT"
"farewell: 'Regards' or TEXT"
"name: FLCAP_TEXT
</pre>

<p>A name must be first-letter capitalized, a formality belongs to a group of specific phrases, and so on and so forth. If we keep up this definition process for long enough, we will at some point have described an email well enough for our purposes. Then we could start parsing emails and do something with it.</p>

<p>The resulting rules for writing this sort of email is called a “domain specific language” or DSL for short.</p>

<p>Notice how the rules above are written in an abstract and beautiful way that resembles mathematics? This makes the system simpler to deal with; we no longer need to write an involved parser program, but can instead just write concise rules in a config file.</p>

<h1 id="experimental-physics-cross-over">Experimental physics cross-over</h1>

<p>With all this in mind, let’s have a look at a real life meta-programming example.</p>

<p>Imagine an experimental physicist with sporadic access to highly expensive and complex equipment. This “instrument” must be configured exactly right for the experiment to be a success - but we don’t have access to it beforehand.</p>

<p>In order to solve this connundrum, experimental physicists will often resort to simulation. We simulate the experiment in software, allowing us to tweak the setup and plan the experiment. The simulation could even show you how to configure the equipment optimally, which is great, then we don’t have to spend as much time tuning the instrument on-site.</p>

<p>Initially, people wrote the whole simulation program from scratch - until standardized and reliable options were developed.</p>

<p>One of these options is McStas.</p>

<p>To keep it all neat and tight, it uses BISON/FLEX to define a text parser. This parser would convert two types of configuration files - components and instruments - into finished simulation programs. The rules that governe these config files is what is meant by “the McStas DSL”.</p>

<h1 id="mcstas-is-not-optimized-for-my-kind-of-change">McStas is not optimized for (my kind of) change</h1>

<p>This software strategy was a great success, and still is. But like most software it has its issues and limitations and a few vectors of potential improvement.</p>

<p>Presented below are three major issues with the system as-is.</p>

<p>The FLEX/BISON setup is rather convoluted. In order to use it, you need to keep track of grammar-rules (.y) and lexer (.l) files. The flex/bison command-line tools take these configuration files as their input, and outputs the C code which comprise the actual DSL parser.</p>

<p>This takes a few steps, so when <em>debugging</em> we have to go through the entire build process:</p>

<ul>
  <li>1) edit the flex/bison configuration files</li>
  <li>2) execute flex/bison cli tools</li>
  <li>3) include the generated parser code into the ‘mcstas’ tool and compile</li>
  <li>4) run the generated mcstas parser-tool on a sample instrument file</li>
  <li>5) the mcstas parser how outputs source code for the simulation</li>
  <li>6) compile the simulation source</li>
  <li>7) run the executable and check this simulation’s output files (using other tools)</li>
</ul>

<p>The process is automated, but we might be hard pressed to use this level of automation and still retain a convenient/efficient development setup.</p>

<h3 id="problem-a-the-lack-of-a-convenient-development-loop">Problem a) The lack of a convenient development loop</h3>

<p>The build process is lengthy and inconvenient for tight development loops, and quite hard to debug.</p>

<p>We can’t debug the parser itself since flex/bison files can’t be debugged and the resulting parser code is incomprehensible and not to be trifled with.</p>

<p>The generated simulation code is monolithic and hard to read and navigate.</p>

<h3 id="problem-b-error-messages-are-very-generic">Problem b) Error messages are very generic</h3>

<p>Another consequence of using flex/bison is that the error codes are quite generic.</p>

<p>This means that when users work with their instruments they may have a hard time figuring out what that error message even means. The line numbers are generated, but beyond a few special cases, there isn’t any more information to go by.</p>

<p>It isn’t terrible, but still leaves a lot to be desired in terms of usability.</p>

<h3 id="problem-c-inaccessible-simulation-core-algorithms">Problem c) Inaccessible simulation core algorithms</h3>

<p>The biggest problem with the current mcstas setup from a developer’s perspective, is the inaccessibility of the core algorithms. For years, mcstas has run the same simulation original ray tracing loop.</p>

<p>At some point, MPI was introduced, and it has also been ported to GPUs with a tool called OpenACC. To make the OpenACC port possible, we had to heavily edit the code generator at the time, again using the long-winded process sketched above. These changes brought performance improvements, but no now capabilities, and came at a significant complexity cost.</p>

<p>My opinion here is that when experimenting with core algorithms is inconvenient, it happens less often.</p>

<h1 id="can-legacy-code-be-salvaged">Can legacy code be salvaged?</h1>

<p>Writing a new McStas DSL parser and code generator seemed unfeasible or not worth it.</p>

<p>Instead, people built tools on top of the core and developed onion rings of software stack - as you do. The basic simulation code that did all of the actual simulation work, seemed to rest quietly below, neatly wrapped away, steeped in CMake and Python. Notable innovative exceptions is the Union components and advanced event logging.</p>

<p>My critique isn’t that the people aren’t willing or able to innovate - but that it becomes harder to do so as time and stack layers build up, and knowledge is invested elsewhere. Which is arguably a shame. We know how innovative ingenious people can be when the proper tools are available.</p>

<p>At some point I needed a programming challenge, and started writing a parser for the McStas DSL from scratch in C. This was during the 2020 pandemic. Writing it was a delightful experience, so a few months ago I took on the task of modernizing that code. Aiming at a serious prototype and to present a viable, alternative take on the mcstas core layer.</p>

<h3 id="testing-the-custom-parser">Testing the custom parser</h3>

<p>A working version of my custom parser is available here: <a href="https://github.com/jakob-garde/mcparse">mcparse</a></p>

<p>Parser output on the bulk data: <a href="https://github.com/jakob-garde/mcparse/blob/main/example_output_0.1.0.md">mcparse/example_output_0.1.0.md</a></p>

<p>Examples messages compared with mcstas: <a href="https://github.com/jakob-garde/mcparse/blob/main/failtest_output_0.1.0.md">mcparse/failtest_output_0.1.0.md</a></p>

<h1 id="what-custom-code-brings-to-the-table">What custom code brings to the table</h1>

<p>Reading the generated mcstas simulation code it becomes clear that it is monolithic and not nearly as modular as it could be. For example, it generates types and initialization code, instead of extracting this into helper libraries. All library code is in-lined and there are duplicate sections targetting various hardware platforms.</p>

<p>Another issue is the way that geometry is handled internally. Today, pepole always use 4x4 affine transformation matrices for their 3D code, but the mcstas implementation used 3x3 rotaion matrices and a separate transformation vector. This is just harder to scrutinize and work with.</p>

<p>Since the parser was already halfway done, I decided to build a code generator into it. It outputs very modular code: One .h file for each component plus one meta file in total, and one .h file for each instrument. The rest is library code. These generated files can be included ad-lib into any project along with now-salvaged runtime simulation libraries.</p>

<p>The salvaged McStas static core runtime simulation code is found here: <a href="https://github.com/jakob-garde/mctrace/blob/main/simcore/simcore.h">simcore.h</a> and <a href="https://github.com/jakob-garde/mctrace/blob/main/simcore/simlib.h">simlib.h</a>.</p>

<p>Another project called <a href="https://github.com/jakob-garde/mctrace">mctrace</a> was developed to demonstrate runtime viability of the code generated by this alternative strategy. It combines custom algorithms with code-generated component- and instrument code. It produces real-time simulation output and mcdisplay- and mcplot-like visuals within a simple wireframe-based 3D user interface.</p>

<p>The main point was to develop, test and validate the code generated by mcparse. It also features modernized component initialization using 4x4 affine transformations, custom visualization hooks for mcdisplay/mcplot, and exemplifies how to write custom trace algorithms.</p>

<h1 id="liberate-verified-physics-code">Liberate verified physics code</h1>

<p>Years and decades has gone into writing, testing and verifying the mcstas components, which make them quite valuable. Unfortunately these gems were not readily available to use in different contexts.</p>

<p>Using the present results, developers with C knowledge can write their own, custom ray-tracing procedures for McStas. Every ported component and instrument is available as a C++ source file, and porting more components is straight-forward.</p>

<p>The mcparse/mctrace project is imited as any early version, but still - most hard problems and road-blocks have been solved: Demonstrated pathways to configure and combine as you please, apply algorithms, frameworks, methods, runtime data analysis’, custom visuals and data formats, all with full C++ compliance.</p>

<p>Adding advanced capabilities to a complex system becomes possible when the code is readily available, modular, quick and easy to run and debug, and does so without any required frameworks or build systems.</p>

<p>The physics is the same, yet now with many more possible capabilities.</p>

<p>So, what’s the catch? The production/release version is still on the horizon. Realizing it would require a degree of funding and community support. So far the project has been a lot of fun and very satisfying, I learned a whole lot, and hereby consider the McStas DSL wheel re-invented!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Text parsing is fun]]></summary></entry></feed>