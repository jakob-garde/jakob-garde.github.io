---
title: "Manually parsing the McStas DSL"
---


Text Parsing is Fun

Recently while researching and learning more C, I happened upon custom parsing and thought it was pretty cool. Turns out that people have been parsing many things manually since way back when, and done it very well.

Parsing sometimes gets a bad rep afterall, and seems like a thing that "should just work", something we should have tools for.

Plenty tools were indeed written during the Bell Labs era and given names like Yacc or (later on) BisonP. These were ancestors of the infamous GNU, one of the most hoofed software animal coming out of the 1980's.

Yet at closer look, parsing is not just about reading JSON files or writing custom compilers, supposedly a common practice in those days. It probably owes its fame to one of the wildest  programming paradigms: META programming.

In an attempt to explain the importance of meta programming to a non-programmer, I'd point to the widespread usage of scripting languages like Python, JavaScript, Ruby, and Perl. All very wide-spread and with a feature called "reflecting" which is notably lacking in C.


What is META Programming?

Meta programming means writing a program that writes ANOTHER program. This is admittedly a two-step process: First you write and execute a program that outputs the source code for another program, which is compiled and executed separately, and often automatically.

With meta programming we may configure certain things abstractly, let the meta process take its course, and reap the automated benefits, rather than writing everything "down-stream" out in tedious detail, tedious - but necessary.

If this sounds a bit complicated, well, it is: Meta programs are incomprehensible to anyone but their authors. Meta programs remind me of convoluted math proofs that skip way too many steps.

Despite the potential for obscurity, meta programming comes with enormous power, and allows us to do many things that are quite clever and time-saving.


Are we still using it?

Also McStas, a physics simulation package originally dating from 1997, was indeed written by people wielding the power of META. As a physics simulation, performance requirements dictated it to be written in C. Thus the meta program had to output C code. Why not write the meta program in C, too? This was not as uncommon as it sounds, back then, and to this day, C is very dependable and portable.

So from a certain perspective it was a pretty great choice. However let's circle back to those furry software animals mentioned above, specifically BISON and FLEX.

The BISON/FLEX combo is known as a parser generator. It is not just a parser, nor just a code generator. It is a parser that uses code generation to generate a parser. Spoken plainly, it is a C program that takes a few configuration files and outputs C source code for a parser.


Overly Contrived Example

In the BISON/FLEX system you write rules that dictate how input text must be structured to be accepted as a valid meta program - meaning the allowed words, sentences and sequences.

For example, let's insist that an email must contain the following elements:

"email: greeting body farewell"

Breaking it down further, the "greeting" could be defined as

"greeting: formality name"
"body: TEXT"
"farewell: formality name"

With this, we must go on to define what qualifies as a "formality" and a "name". A name must be capitalized, a formality belongs to a group of specific phrases, and so on and so forth. If we keep up this definition process, we will at some point have described an email well enough for our purposes.

This is what's called a "domain specific language" or DSL for short.

Notice how the rules above are written in an abstract and beautiful way that resembles mathematics. Amazing, isn't it?


Experiment Physics Cross-over

Imagine an experimental physicist with sporadic access to highly expensive and complex equipment. This "instrument" must be configured exactly right for the experiment to be a success - but we don't have access to it beforehand.

In order to solve this connundrum, we resort to simulation. We simulate the experiment in software, allowing us to tweak the setup and plan the (virtual) experiment. The simulation could even show you how to configure the equipment optimally, which is great, then we don't have to spend (as much) time tuning the instrument on-site.

Since simulation code is fickle and complicated, we use-existing software components which match the physical instrument as closely as possible. We don't want to write the whole simulation program from scratch - although people in the real reality actually did for years - until better options was developed. One of these "better options" is called McStas.

In other words, people created easier ways to configure their simulation in terms of pre-written software components. To keep it all neat and tight, they used BISON/FLEX to create the parser that would make all this happen.

This parser would convert two types of configuration files - components and instruments - into finished simulation programs. This tool is then called mcstas (and mcxtrace), and the  rules that govern how these configuration files must be written, is called the McStas DSL.


HER


McStas is great, but ...

This software strategy was a great success, and there is nothing bad to say about it, really. As all software projects though, it has its issues and unwanted limitations, as well as vectors of potential improvement.

Presented below are three major issues with the system as-is, as well as a proposed fix.

The FLEX/BISON setup is rather convoluted. In order to use it, you need a few "grammar rules" and "lexer" files, which must be stored in the project. The flex/bison command-line tools take these configuration files as their input, and outputs the C code which comprise the actual DSL parser.

This takes a few steps, so when debugging any changes to the configuration, you have to go through the entire build process:

1) edit the flex/bison configuration files ->
2) execute flex/bison cli tools ->
3) include the generated parser code into the 'mcstas' tool and compile ->
4) run the generated mcstas parser-tool on a sample instrument file ->
5) the mcstas parser how outputs source code for the simulation ->
6) compile the simulation source ->
7) run the executable and check this simulation's output files (using other tools)

This process can be automated, sure, but we may be hard pressed to use this level of automation and still retain a convenient development setup.

Problem a) The lack of a convenient development process.

We also can't debug the parser, because the flex/bison .y and .l files can't be debugged, and the resulting parser code is incomprehensible, very recursive and nested, and not to be trifled with.

Problem b) Error messages are very generic

Another consequence of this degree of automation, is that the error codes the generated parser outputs are quite generic.

This means that when new (and even experienced) users work with their instruments, they may have a hard time figuring out what that cryptic error message even means. Sure it isn't terrible, but it leaves a lot to be desired in terms of usability.

Problem c) Inaccessible simulation core algorithms

The biggest problem with the current mcstas setup, in my personal opinion, is the inaccessibility of the core algorithms.

For years, mcstas has run essentially the same simulation ray tracing loop. At some point, MPI was introduced, and it has also been ported to GPUs with a tool called OpenACC. To make the OpenACC port possible, we had to heavily edit the code generator, again using the long-winded process sketched above.

It bears mentioning that the mcstas code generator is indeed tightly linked to the parser, since part of the job of the parser is to collect data, and set it up in a convenient way for later code geneartion.

Both brought significant performance improvements, but came at a steep complexity cost.

Experimenting with, and changing the core algorithms was a slow and arduous process. If we, the core developers, couldn't do it, it was a sure bet that the users wouldn't. Changes to the parser/DSL were atomic and sporadic, and core algorithms stayed essentially the same.


Can such a Legacy be "Fixed"?

The assumption was always that writing a parser - and thus another code generator as well - in any other way, was either unfeasible or unviable in terms of time invested.

So for 25 years, the ring build-system lay dormant at the bottom of the Rhine, awaiting the programmers to reclaim it ...

People have thus built tools ON TOP OF the un-touchable mcstas core and developed onion rings of software stack on top of it. (As you do.) And mostly these days, the basic simulation tools seem to be happily ignored, which is presumably fine for the present purposes.

What about those forgotten ground-layers or code, though? For whatever reason, I started writing a parser for the mcstas DSL during the Corona crisis. It was such a delightful experience, and a few months ago I took up the project and continued the work.

Finally today, we have a working prototype of 'mcparse', a custom mcstas DSL parser with 95% language coverage.

To prove its validity, I wrote a code generator to match. In the 'mctrace' project, this generated code is combined with other custom algorithms and tools to produce real run-time simulation output and graphics.

It combines partially with the mcstas simulation core, also salvaged from the McCode project, and with custom ray-tracing procedures. This is described elsewhere, for now just an assertion that a working, proof-of-concept code-generator and simulation core exists.


What Custom parsing brings to the table


Here's to the example outputs





