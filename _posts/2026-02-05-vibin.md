---
title: "Vibing Guidelines"
---


*Remember coding guidelines? In the era of LLMs, coding is so 2025 and vibe coding is clearly the thing to do now. But how are we supposed to do it right? The lack of Vibe Coding Guidelines (TM) may lead to security issues, the lack of backups, accidental database deletion or sensitive data leaks!*

LLMs can be a useful tool if handled correctly.

Let's get into it.


# Pitfalls


We have all been there, read the hype articles, and the doom posts and so on and so forth. This is neither. To use any tool effective, we need to abide by certain principles. This is nothing new, and applies to everything from toothbrushes to chainsaws. In terms of using LLMs to generate code, many experienced developers have probably tried it and gotten lackluster results.

Here's my take on their usage for coding. First, let's take on the elephant in the room: Some of them are kind of sleazy.

- LLMs tend to endlessly stroke our egos.

Responses are verbose and full of fluff like "great idea". Don't fall for it, it is just trying to draw us in. We are better off keeping our egos out of the engineering process.

- LLMs can't do more than one thing at the time without being wrong.

The system will rarely say "does not compute" if our prompt isn't specific enough. The more stuff it outputs, the more work this causes for us; in either reading and ignoring the output, or in accepting, verifying and testing it.

- LLMs don't see the big picture.

According to the hype disclaimer above, LLMs never demonstrated any real sense of purpose. The bigger the scope of a prompt, the harder it becomes to get a practical answer. We need to hold the purpose ourselves or risk getting sidetracked by less-than-relevant suggestions.

- Following LLM suggestions often paints us into a corner.

The more suggestions we accept, the more we ma end up in an LLM-generated box and find ourselves in a bad spot, unable to backtrack. Thus we need to start over, losing time. Either that or we may forget to verify, losing time on something that was just plain wrong.

Notice how losing time is a big pitfall when using these productivity-enhancing tools?

Let's fix it. The key is to take control of the process, just as we would using a screwdriver or coffee machine. (Clearly, we can't let the coffee machine decide when it is time for coffee.)


# Strengths


The LLM is great at being "approximately right" so let's use this to our advantage.

- Use LLMs to summarize practices within common fields of knowledge.
- Use LLMs to present well-known solutions at an appropriate (controlled, escalating) level of detail.

We can use it to get the gist of a field of knowledge or problem domain. It doesn't have to be exactly right here - but it will be much less reliable in niche areas. We should try and verify only the most promising avenues as early as possible.

Think of the exploration as walking a tree structure that goes from general at the root, to specific at the branches. We need to control the path according to our current purpose and understanding. These are our modes of operation:

- General, exploratory 
- Specific, solution-oriented

We should step up and down the knowledge tree, selecting detailed topics as we descend, or "stepping back" up the tree if the current topic seems less promising.

With this in mind, lets formulate some guidelines a bit more concisely.


# Guidelines


First, let's get rid of the fluff and some of the side-tracking tendencies:

*"Please keep your responses extremely concise, objective and in third-person, focusing on the information."*

This configures the LLM to respond like a dry, interactive textbook, which is perfect for solving engineering problems. If it still comes up with too many side-tracking suggestions, use something like this:

*"Please reduce the number of suggestions generated in responses, keeping it concise and to the point."*

Be polite and punctual, this maintains distance and good form. The LLM is not our friend, we should think of them as a knowledge resource to be accessed sparingly and at our own risk.

Second, our purpose must be held central in mind - but keept to ourselves! We delegate the unknowns to the machine, but don't try to make it "understand" our intentions as a human being would.

Take control of the process instead. We do this by using the two modes of operation mentioned above to step up and down the knowledge tree:

- "List some of the libraries used for ..."
- "What are the techniques commonly used withing the field of ..."
- "Describe ... in more detail"
- "Clarify how ... relates to ..."
- "What are some of the potential problems in combining ... with ..."

Having gotten an overview, we may look into some specifics:

- "Give an example of how to use library ... to do ..."
- "How would I generate a code context able to handle ..."
- "What does a for-loop look like in Python?"

An LLM will automatically fill in the blanks, so let's not give it any blanks. It does not understand what we are trying to do here. Rather, it is our job to operate the machine correctly in the service of our purpose.

The more specific, solution-oriented we get, the more we need to copy-paste, modify and compile any generated example snippets for verification. We always want output that can be read, understood, tested and verified. Thus we must be very specific in our prompts, and keep its responses in check.

Once we have something tested, edited and verified in our own code base, we should use *that* as the basis to proceed; reading online docs, using search engines, etc. Leave the LLM alone at the earliest opportunity. We got what we came for, best let sleeping dragons be.


